----------
Quickstart
----------

This document will provide you with an introduction on how to:

* Set up a basic workspace with SESEMI.
* Use the `open_sesemi` command-line interface to run built-in training configurations.
* Train a model using semi-supervised learning and visualize the progress.
* Evaluate the trained model.

..
  Audience

  * Have a high-level understanding of the different components of SESEMI.
  * Have installed SESEMI in a virtual environment and made sure that it works.

  Goal

  * Users have exposure to each component described in the overview.
  * Users know about the existence of the different training configurations
    that are built-in and how to inspect them via the command line.
  * Users are familiar with training a barebones model using a built-in config end-to-end.
  * Users know about how to configure a basic workspace for using SESEMI.
  * Reference the tutorials for further details.

  Outline

  * Workspace setup.
  * open_sesemi command-line description and inspection.
  * End-to-end training of semi-supervised baseline model (imagewang? or other dataset).
  * Visualization of the training logs.
  * Evaluation on a test set.

===============
Workspace Setup
===============

After setting up a working environment for SESEMI using either pip or docker,
you can configure a workspace to store data, logs, model artifacts, and more.

The recommended structure for projects using SESEMI is elaborated further in the tutorials,
however, we present a basic outline here::

  /experiments       # Stores your code, configurations, data, metrics, and models.
    /data            # Any datasets you want to use. Symlink data directories as necessary.
    /runs            # Stores models and metrics generated by SESEMI.
    /configs         # Your custom Hydra configurations.
    /src             # Custom code with packages that you can instantiate from the configs.
      /package_a     #   If you don't use a setup.py file to install these packages
      /package_b     #   you can instead configure your PYTHONPATH to point here as well.
    setup.py         # An optional setup file to install the packages in editable mode.

As we will not be using custom configs and source code here, you may ignore those for now.

===========
Open SESEMI
===========

The main entrypoint used for training models is the built-in `open_sesemi` command-line interface (CLI).
Alongside it, we have packaged training configurations that can be used to reproduce results
or invoke on modified datasets or models.

The CLI is built on top of Hydra, which may be different from what you are used to.
A key departure from most CLIs is that rather than just setting top-level flags,
Hydra enables setting nested parameters of configuration objects using a standard dot notation and the
assignment operator (e.g. `a.b=1`). This is explained in more depth in the tutorials, however,
it is sufficient to be aware of this for now.

^^^^^
Hydra
^^^^^

To get details on how you can use Hydra CLIs differently, you can also run:

.. command-output:: open_sesemi --hydra-help

One thing to note is that you can install tab completion capabilities for various shells by invoking one
of the enumerated installation commands. For example to configure shell completion with bash:

.. code-block:: Bash

  $ eval "$(open_sesemi -sc install=bash)"

^^^^^^^^^^^^^^^^
General Commands
^^^^^^^^^^^^^^^^

To visualize the help text specific to the program you can run:

.. code-block:: bash

  $ open_sesemi -h

For more detailed information on the program you can run:

.. code-block:: bash

  $ open_sesemi -i

To show the configuration being used:

.. code-block:: bash

  $ open_sesemi -c all    # Everything inclduing Hydra-specific configs.
  $ open_sesemi -c job    # Just the config of your job.
  $ open_sesemi -c hydra  # Just the Hydra-specific config.

To run a specific config using the default config path:

.. code-block:: bash

  $ open_sesemi -cn ${CONFIG_NAME}

To run a specific config by adding another directory to the config search path:

.. code-block:: bash

  $ open_sesemi  -cd ${CONFIG_DIR} -cn ${CONFIG_NAME}

=======================
Baseline Model Training
=======================

Included with SESEMI are a number of tested training configurations that can be used to
train baseline supervised and semi-supervised models. These are stored
under `sesemi/trainer/conf` within the codebase and are elaborated on further in a dedicated section of the docs.
Here we just describe how to use one such configuration file.

In particular, we will be training a model on the imagewoof dataset by fast.ai.
From the workspace that you set up earlier (experiments directory), follow these instructions:

1. Download and extract the imagewoof2 dataset to the data directory.

.. code-block:: bash

  $ curl https://s3.amazonaws.com/fast-ai-imageclas/imagewoof2.tgz | tar -xzv -C ./data

2. Run training using SESEMI for 80 epochs.

You should get 90-91% accuracy on the imagewoof2 dataset, which is competitive on the `FastAI leaderboard <https://github.com/fastai/imagenette#imagewoof-leaderboard>`_, using a standard training protocol + unlabeled data, without fancy tricks.

.. code-block:: bash

  $ open_sesemi -cn imagewoof_rotation

The training logs with all relevant training statistics (accuracy, losses, learning rate, etc.) are written to the `./runs` directory. You can use `TensorBoard <https://www.tensorflow.org/tensorboard>`_ to view and monitor them in your browser during training.
  
.. code-block:: bash

  $ tensorboard --logdir ./runs
    
3. Run evaluation on the trained checkpoint.

.. code-block:: bash

  $ CHECKPOINT_PATH=$(echo ./runs/imagewoof_rotation/*/lightning_logs/version_0/checkpoints/last.ckpt)
  $ open_sesemi -cn imagewoof_rotation \
      run.mode=VALIDATE \
      run.pretrained_checkpoint_path=$CHECKPOINT_PATH

Now that you have a taste for training models with SESEMI, you can move on to the tutorials to
learn more about how it works and different ways to customize it for your data.