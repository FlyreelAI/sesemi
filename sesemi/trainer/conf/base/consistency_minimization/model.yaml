# @package _global_
learner:
  hparams:
    model:
      ema:
        decay: 0.999
      regularization_loss_heads:
        consistency_minimization:
          head:
            _target_: sesemi.models.heads.loss.EMAConsistencyLossHead
            input_data: unsupervised
            student_backbone: supervised_backbone
            teacher_backbone: supervised_backbone_ema
            student_head: supervised_head
            teacher_head: supervised_head_ema
            loss_fn: mse
          scheduler:
            _target_: sesemi.schedulers.weight.SigmoidRampupScheduler
            weight: 10.0
            stop_rampup: 12000
          scale_factor: 1.0
        
