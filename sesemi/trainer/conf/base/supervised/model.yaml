# @package _global_
defaults:
  - /base/config
run:
  strategy: dp
  accelerator: gpu
  num_epochs: 80
learner:
  hparams:
    model:
      loss:
        head:
          _target_: sesemi.models.loss_heads.SupervisedLossHead
          loss_fn:
            _target_: torch.nn.CrossEntropyLoss
            reduction: none
trainer:
  sync_batchnorm: true
  precision: 16
  log_every_n_steps: 50
  deterministic: true
  callbacks:
    - _target_: pytorch_lightning.callbacks.ModelCheckpoint
      monitor: val/top1
      mode: max
      save_top_k: 1
      save_last: True
    - _target_: pytorch_lightning.callbacks.DeviceStatsMonitor
