defaults:
  - sesemi_config
  - learner/classifier
run:
  seed: 42
  num_epochs: 80
data:
  train:
    supervised:
      _target_: sesemi.DataLoader
      dataset:
        _target_: sesemi.dataset
        name: image_folder
        root: /home/appuser/sesemi/data/imagewoof2
        subset: train
        image_transform:
          _target_: sesemi.transforms.train_transforms
      shuffle: True
      batch_size: 32
      num_workers: 4
    rotation_prediction:
      _target_: sesemi.DataLoader
      dataset:
        _target_: sesemi.dataset
        name: image_folder
        root: /home/appuser/sesemi/data/imagewoof2
        image_transform:
          _target_: sesemi.transforms.train_transforms
      shuffle: True
      batch_size: 32
      num_workers: 4
      pin_memory: True
      collate_fn:
        _target_: sesemi.collation.RotationTransformer
      drop_last: True
  val:
    _target_: sesemi.DataLoader
    dataset:
      _target_: sesemi.dataset
      name: image_folder
      root: /home/appuser/sesemi/data/imagewoof2
      subset: val
      image_transform:
        _target_: sesemi.transforms.center_crop_transforms
    batch_size: 32
    shuffle: False
    pin_memory: True
    num_workers: 4
    drop_last: False
learner:
  classes:
  - n02086240
  - n02087394
  - n02088364
  - n02089973
  - n02093754
  - n02096294
  - n02099601
  - n02105641
  - n02111889
  - n02115641
  model:
    backbone:
      _target_: sesemi.PyTorchImageModels
      name: resnet50d
      freeze: False
      pretrained: False
      global_pool: avg
      dropout_rate: 0.5
    supervised_loss:
      callable:
        _target_: torch.nn.CrossEntropyLoss
    regularization_loss_heads:
      rotation_prediction:
        head:
          _target_: sesemi.models.heads.loss.RotationPredictionLossHead
          input_data: rotation_prediction
          input_backbone: backbone
  optimizer:
    _target_: torch.optim.SGD
    lr: 0.1
    momentum: 0.9
    nesterov: True
    weight_decay: 0.
  lr_scheduler:
    scheduler:
      _target_: sesemi.PolynomialLR
      warmup_epochs: 1
      iters_per_epoch: ${sesemi:iterations_per_epoch}
      warmup_lr: 0.001
      lr_pow: 0.5
      max_iters: ${sesemi:max_iterations}
trainer:
  gpus: 2
  accelerator: dp
  callbacks:
    - _target_: pytorch_lightning.callbacks.ModelCheckpoint
      monitor: val/top1
      mode: max
      save_top_k: 1
      save_last: True
